<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Safety Framework Guide</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
</head>
<body>
    <header class="page-header">
        <h1>A Guide to Writing Frontier AI Safety Frameworks</h1>
        <div class="metadata">
            <p>Author: Neel Alex</p>
            <p>Contact: alexneel@gmail.com</p>
            <p>Last Updated: November 19, 2024</p>
        </div>
    </header>

    <div class="intro-container">
        <div class="intro-text">
            <p><i>This document is primarily aimed at signatory organizations of the <a href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024">Frontier AI Safety Commitments</a>, though we hope that all AI developers will find this guide useful.</i></p>

            <p>The Frontier AI Safety Commitments at the AI Seoul Summit aim to ensure that frontier AI developers (1) effectively plan to responsibly manage risk from powerful AI systems, (2) develop internal structures to hold themselves accountable for safe development and deployment, and (3) make their safety plans appropriately transparent to external actors. To this end, the 16 signatory companies agreed to publish a document called a safety framework that demonstrates the fulfillment of eight specific commitments before the upcoming AI Summit in France.</p>
            
            <p>In this document, we provide a step-by-step guide to making such a safety framework. We reference the following existing safety frameworks: <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">OpenAI's Preparedness Framework</a>, <a href="https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf">Anthropic's Responsible Scaling Policy</a>, and <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf">Google DeepMind's Frontier Safety Framework</a>.</p>
        </div>

        <div class="commitment-table-wrapper">
         <table class="commitment-table">
           <thead>
             <tr>
               <th>Commitment</th>
               <th>Actions Needed</th>
             </tr>
           </thead>
           <tbody>
             <tr>
               <td>I. Risk Assessments</td>
               <td>
                 <ol>
                   <li>Identify which risks will be assessed.</li>
                   <li>Describe how risks will be assessed.</li>
                   <li>Describe when risks will be assessed.</li>
                   <li>Describe how third-party risk assessments will be incorporated.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>II. Risk Thresholds</td>
               <td>
                 <ol>
                   <li>Set detailed thresholds where risk is intolerable.</li>
                   <li>Describe how trusted actors can help set risk thresholds.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>III. Mitigations</td>
               <td>
                 <ol>
                   <li>Set a process for identifying and implementing risk mitigations.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>IV. Mitigation Strategy</td>
               <td>
                 <ol>
                   <li>Describe how to respond to risks above thresholds.</li>
                   <li>Set thresholds where a model will not be deployed or developed.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>V. Continual Improvement</td>
               <td>
                 <ol>
                   <li>Describe how I-IV will be improved over time.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>VI. Governance</td>
               <td>
                 <ol>
                   <li>Describe an internal governance structure to enforce framework.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>VII. Transparency</td>
               <td>
                 <ol>
                   <li>Determine which sections of the framework can be shared publicly.</li>
                   <li>Describe how trusted actors can see additional framework details.</li>
                 </ol>
               </td>
             </tr>
             <tr>
               <td>VIII. Feedback Opportunities</td>
               <td>
                 <ol>
                   <li>Describe if and how external actors can comment on the quality of the framework and the organization's implementation thereof.</li>
                 </ol>
               </td>
             </tr>
           </tbody>
         </table>
        </div>
    </div>

    <div class="layout-wrapper">
        <!-- Section 1 -->
        <div class="section-wrapper" data-section="1">
            <h2>Commitment 1 (Risk Assessments)</h2>

            <div class="info-box">
                <div class="info-box-header">
                    Frontier AI Safety Commitments, Commitment 1
                </div>
                <div class="info-box-content">
                    <p>I. Assess the risks posed by their frontier models or systems across the AI lifecycle, including before deploying that model or system, and, as appropriate, before and during training. Risk assessments should consider model capabilities and the context in which they are developed and deployed, as well as the efficacy of implemented mitigations to reduce the risks associated with their foreseeable use and misuse. They should also consider results from internal and external evaluations as appropriate, such as by independent third-party evaluators, their home governments, and other bodies their governments deem appropriate.</p>
                </div>
            </div>
            
            <div class="subsection-wrapper" data-subsection="1.1">
                <div class="content-block">
                    <h3>Step 1: Risk Identification</h3>

                <ol>
                    <li>The commitments explicitly require consideration of <strong>Model Theft</strong> of unreleased model weights. Affirm that this risk vector will be considered.
                        <ol>
                            <li>Controls for model weights, both physical and digital, may go hand-in-hand with security controls for other sorts of intellectual property, such as codebases and algorithmic advances.</li>
                        </ol>
                    </li>
                    <li>Potentially consider additional risk vectors as you see fit. This is not explicitly required by the commitments, but existing safety frameworks all consider potential risks downstream of model capabilities.
                        <ol>
                            <li>Generally, consider the range of capabilities of the model and its possible use-cases across deployment contexts. Some questions to consider:
                                <ol>
                                    <li>What hostile actors will be able to use the model? In what ways are they currently prevented from causing harm, and could the model help them overcome those?</li>
                                    <li>In the course of normal use, how could the AI system cause accidental harm to the user? What is the potential scope of this harm?</li>
                                </ol>
                            </li>
                            <li><a href="https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a"><strong>Autonomy</strong>, <strong>ML R&D</strong></a>, <a href="https://googleprojectzero.blogspot.com/2024/06/project-naptime.html"><strong>Cyber Offense</strong></a>, and <a href="https://www.rand.org/pubs/research_reports/RRA2977-1.html"><strong>Bioweapons</strong></a> are currently considered risk vectors in all published safety frameworks.</li>
                            <li>This process requires judgment calls, and will vary depending on your organization.</li>
                                <ol>
                                    <li>We recommend taking into account existing practices as well as feedback from leadership, employees, and the public.</li>
                                </ol>
                            </li>
                        </ol>
                    </li>
                </ol>

                <p>Risk vectors are specific mechanisms by which AI systems could cause harm, and include both deliberate malicious action as well as unintended consequences. Each risk vector may require a unique approach to assessment, monitoring, and mitigation.</p>

                <p>While examining additional risk vectors can reduce the risk of a model causing harm, it also increases costs – requiring additional evaluations (including potentially <a href="https://arxiv.org/abs/2310.11986">more complex evaluations</a>), more mitigation steps, and a potential risk of delaying or stopping deployment. If your organization wants to consider a broader set of harms than are listed here, we recommend looking at the <a href="https://airisk.mit.edu/">AI Risk Repository</a>, which attempts to catalog many more AI harms.</p>

                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 1.1 - Type A</h3>
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque consequat pharetra sem vitae mollis. Quisque viverra erat vel enim scelerisque tempus. Ut sed interdum neque, sed pellentesque mi. Etiam bibendum sem sapien, id posuere lectus laoreet sed. Aliquam eleifend, velit ac faucibus tristique, neque arcu dignissim neque, a ultricies tellus lectus sed sem. Curabitur faucibus risus eu ullamcorper varius. Donec arcu sapien, fermentum quis porta porta, placerat eget justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.</p>
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque consequat pharetra sem vitae mollis. Quisque viverra erat vel enim scelerisque tempus. Ut sed interdum neque, sed pellentesque mi. Etiam bibendum sem sapien, id posuere lectus laoreet sed. Aliquam eleifend, velit ac faucibus tristique, neque arcu dignissim neque, a ultricies tellus lectus sed sem. Curabitur faucibus risus eu ullamcorper varius. Donec arcu sapien, fermentum quis porta porta, placerat eget justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 1.1 - Type B</h3>
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque consequat pharetra sem vitae mollis. Quisque viverra erat vel enim scelerisque tempus. Ut sed interdum neque, sed pellentesque mi. Etiam bibendum sem sapien, id posuere lectus laoreet sed. Aliquam eleifend, velit ac faucibus tristique, neque arcu dignissim neque, a ultricies tellus lectus sed sem. Curabitur faucibus risus eu ullamcorper varius. Donec arcu sapien, fermentum quis porta porta, placerat eget justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 1.1 - Type C</h3>
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque consequat pharetra sem vitae mollis. Quisque viverra erat vel enim scelerisque tempus. Ut sed interdum neque, sed pellentesque mi. Etiam bibendum sem sapien, id posuere lectus laoreet sed. Aliquam eleifend, velit ac faucibus tristique, neque arcu dignissim neque, a ultricies tellus lectus sed sem. Curabitur faucibus risus eu ullamcorper varius. Donec arcu sapien, fermentum quis porta porta, placerat eget justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.</p>
                            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque consequat pharetra sem vitae mollis. Quisque viverra erat vel enim scelerisque tempus. Ut sed interdum neque, sed pellentesque mi. Etiam bibendum sem sapien, id posuere lectus laoreet sed. Aliquam eleifend, velit ac faucibus tristique, neque arcu dignissim neque, a ultricies tellus lectus sed sem. Curabitur faucibus risus eu ullamcorper varius. Donec arcu sapien, fermentum quis porta porta, placerat eget justo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 1.1 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="1.2">
                <div class="content-block">
                    <h3>Step 2: Assessment Description</h3>
                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 1
                        </div>
                        <div class="info-box-content">
                            <p>I. Assess the risks posed by their frontier models or systems across the AI lifecycle… Risk assessments should consider model capabilities and the context in which they are developed and deployed, as well as the efficacy of implemented mitigations to reduce the risks associated with their foreseeable use and misuse...</p>
                        </div>
                    </div>

                    <p>For each risk identified in Step 1:</p>
                    <ol>
                        <li>Describe the type of evaluation that will be used to assess the risk of an AI system. Some examples:
                            <ol>
                                <li>For <strong>Model Theft</strong>: Red-teaming can be a way to judge security measures effectively.</li>
                                <li>For <strong>Bioweapons</strong>: Typical evaluations judge whether an AI system is better than a search engine at aiding:
                                    <ol>
                                        <li>Amateurs in producing known pathogens.</li>
                                        <li>Expert in producing novel pathogens.</li>
                                    </ol>
                                </li>
                                <li>For <strong>Cyber Offense</strong>: Capture-the-flag challenges are often used to estimate a system's cyberoffensive capabilities.</li>
                                <li>Justify how each assessment is relevant to real-world outcomes.
                                    <ol>
                                        <li>For example: if <strong>Cyber Offense</strong> evaluations are developed with experts to mimic real-world codebases not in the training data, an AI system's performance on the assessment represents real hacking capability.</li>
                                        <li>To ensure external validity and prevent "teaching to the test", perhaps detailed information on what is assessed should be limited even within a given organization.</li>
                                    </ol>
                                </li>
                                <li>Options exist for non-capability-based evaluations for certain risk domains. For example:
                                    <ol>
                                        <li>Scaling law analyses might be used to predict how <strong>Model Autonomy</strong> will increase without actually testing models.
                                            <ol>
                                                <li>However, this method may not be useful for judging <strong>Bioweapons</strong> risks, since narrow AI systems can still aid in dangerous tasks.</li>
                                            </ol>
                                        </li>
                                        <li>Analyses of the training data to detect presence of examples of dangerous capabilities could be used to filter out <strong>Bioweapons</strong>-relevant data.</li>
                                        <li>Forecasting of future capabilities can also be used to predict AI system capabilities, as mentioned in OpenAI's Preparedness Framework.</li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                        <li>Describe how this type of evaluation can give information about how close to a risk threshold a given AI model is. This is required in Commitment 2.
                            <ol>
                                <li>For instance: in <strong>Cyber Offense</strong>, capture-the-flag challenges are often rated by difficulty. Completing more advanced challenges can signal that a model is approaching a risk threshold.</li>
                                <li>In Anthropic's RSP v1, <em>fraction</em> of tasks completed in a given evaluation domain area is also taken as an indicator of growing model capabilities.</li>
                            </ol>
                        </li>
                        <li>Provide any additional details on how the evaluations will be run. For instance:
                            <ol>
                                <li>Which capability elicitation techniques will be used.
                                    <ol>
                                        <li>Fine-tuning, agent scaffolding, external tools, etc.</li>
                                    </ol>
                                </li>
                                <li>Which mitigations will be in-place, and how will they be tested against?
                                    <ol>
                                        <li>Will base models be tested, or only instruction-tuned/RL-tuned?</li>
                                        <li>If, e.g. refusals are being used, how will evaluation prompts be picked? Can they be adversarially tuned?</li>
                                    </ol>
                                </li>
                            </ol>
                        </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 1.2 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 1.2 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 1.2 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 1.2 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="1.3">
                <div class="content-block">
                    <h3>Step 3: Assessment Schedule</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 1
                        </div>
                        <div class="info-box-content">
                            <p>I. Assess the risks posed [...], including before deploying that model or system, and, as appropriate, before and during training.</p>
                        </div>
                    </div>

                    <ol>
                       <li>State that assessments will be done before deployment.
                           <ol>
                               <li>Optionally, set a threshold below which models are likely to be harmless, and require no assessment.
                                   <ol>
                                       <li>This should adapt to the model use-case. If the threshold is based on compute, general-purpose systems could be considered safe below the threshold, but narrow biological systems could still yield dangerous results below the the threshold.</li>
                                   </ol>
                               </li>
                           </ol>
                       </li>
                       <li>Determine how often assessment will be run during training.
                           <ol>
                               <li>Typically, organizations assess every time the effective compute put into a system increases by a multiplicative factor.</li>
                               <li>For example: Every time the amount of compute invested in training the system grows by 4x over the previous time the system was assessed, a new assessment must be done before training can continue.</li>
                               <li>Specific details depend on an organization's needs. Some questions to consider:
                                   <ol>
                                       <li>How costly are assessments to run?</li>
                                       <li>Will only a subset of assessments be run mid-training?</li>
                                       <li>At a given compute multiplier, how often would assessments be run?</li>
                                   </ol>
                               </li>
                           </ol>
                       </li>
                       <li>Determine if assessments will be done <em>before</em> training, and if so, how.</li>
                       <li>Set a schedule for post-deployment assessment as necessary. Examples:
                           <ol>
                               <li>Regularly scheduled testing of defenses against <strong>Model Theft</strong>.</li>
                               <li>Evaluating improvements due to ongoing fine-tuning or improved scaffolding.</li>
                           </ol>
                       </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 1.3 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 1.3 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 1.3 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 1.3 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="1.4">
                <div class="content-block">
                    <h3>Step 4: Third-Party Risk-Assessments</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 1
                        </div>
                        <div class="info-box-content">
                            <p>I. [...] They should also consider results from internal and external evaluations as appropriate, such as by independent third-party evaluators, their home governments, and other bodies their governments deem appropriate.</p>
                        </div>
                    </div>

                    <ol>
                       <li>Describe which types of third party organizations will be involved in risk assessment. For example:
                           <ol>
                               <li>Computer security red-teaming orgs.</li>
                               <li>Third party eval-providing organizations, e.g. METR, Apollo.</li>
                               <li>Government institutes, e.g. US AISI, UK AISI, EU AI Office.</li>
                           </ol>
                       </li>
                       <li>Describe what role the third party organizations will play, and how additional third party organizations can get involved. Some details to consider specifying:
                           <ol>
                               <li>What risk vectors will they assess?
                                   <ol>
                                       <li>Certain assessments may be better executed in-house, due to an organization's particular needs.</li>
                                   </ol>
                               </li>
                               <li>When will they participate in testing?</li>
                               <li>How will their results be incorporated into mitigation strategies?</li>
                           </ol>
                       </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 1.4 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 1.4 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 1.4 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 1.4 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

                <!-- Section 2 -->
        <div class="section-wrapper" data-section="2">
            <h2>Commitment 2 (Risk Thresholds)</h2>
            
            <div class="subsection-wrapper" data-subsection="2.1">
                <div class="content-block">
                    <h3>Step 1: Risk Thresholds</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 2
                        </div>
                        <div class="info-box-content">
                            <p>II. Set out thresholds at which severe risks posed by a model or system, unless adequately mitigated, would be deemed intolerable. Assess whether these thresholds have been breached, including monitoring how close a model or system is to such a breach. [...] They should also be accompanied by an explanation of how thresholds were decided upon, and by specific examples of situations where the models or systems would pose intolerable risk.</p>
                        </div>
                    </div>

                    <ol>
                       <li>Gather relevant stakeholders or information needed to set thresholds. For example:
                           <ol>
                               <li>The organization's leadership.</li>
                               <li>Representative of the home government.</li>
                               <li>Information about national laws or international agreements.</li>
                               <li>(Optional) information about other parties' interests, e.g. survey results of the general public's preferences.</li>
                           </ol>
                       </li>
                       <li>Determine <em>how</em> thresholds will be set. Capability thresholds are the most common form of threshold.
                           <ol>
                               <li>However, other industries often explicitly calculate an expected harm (e.g. in dollars of damages or lives lost) and adjust development and deployment according to <a href="https://arxiv.org/abs/2406.14713">corresponding quantitative thresholds</a>.</li>
                           </ol>
                       </li>
                       <li>For each identified risk category from Commitment 1, determine a degree of risk that would be intolerable. These are the risk thresholds. For example:
                           <ol>
                               <li>For <strong>Model Theft</strong>, the risk might be intolerable if an adversary could exfiltrate a given model's weights with an attack that costs <10% of the model's training cost.
                                   <ol>
                                       <li>We recommend organizations consult the <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html">RAND report on Securing AI Model Weights</a>, which describes a number of concrete thresholds of security to consider when deploying frontier AI systems.</li>
                                   </ol>
                               </li>
                               <li>For <strong>Cyber Offense</strong>, the risk might be intolerable if the model could identify a zero-day and take control of a given arbitrary EC2 instance.</li>
                               <li>For <strong>Bioweapons</strong>, the risk might be intolerable if the model would enable a PhD-level expert to synthesize a pandemic-potential pathogen with less than $100,000.</li>
                               <li>These risk-thresholds might need to be set based on the intended deployment modality. The risks associated with serving an API of a base model might be different than the risks associated with offering fine-tuning.</li>
                           </ol>
                       </li>
                       <li>For each threshold set in the previous step, briefly justify why it was chosen.
                           <ol>
                               <li>Perhaps as simple as cleaning up internal discussions.</li>
                           </ol>
                       </li>
                       <li>For each justification above, describe in moderate detail the corresponding scenario where, if the threshold is exceeded and the risk isn't mitigated, intolerable harms can occur.</li>
                       <li>Optionally, set additional thresholds as necessary to ensure that AI systems are safely developed and deployed. These may not need to be fully fleshed out as the intolerable risk thresholds above. For example:
                           <ol>
                               <li>If some mitigations are expensive to apply, at which threshold will they start coming into practice?</li>
                               <li>Since safety mitigations such as refusals can be nearly-trivially removed by <a href="https://arxiv.org/abs/2311.00117">adversarial fine-tuning</a>, thresholds based on the model's performance before safety mitigations may be useful to address threat models such as <strong>Model Theft</strong>.</li>
                           </ol>
                       </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 2.1 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 2.1 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 2.1 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 2.1 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="2.2">
                <div class="content-block">
                    <h3>Step 2: Input from Trusted Actors</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 2
                        </div>
                        <div class="info-box-content">
                            <p>II. [...] These thresholds should be defined with input from trusted actors, including organisations’ respective home governments as appropriate. They should align with relevant international agreements to which their home governments are party.</p>
                        </div>
                    </div>

                    <ol>
                       <li>Affirm that home governments will be involved in setting thresholds of intolerable risk.</li>
                       <li>List any other parties that will be involved in threshold setting.
                           <ol>
                               <li>Optionally, describe how each actor contributed, e.g. by making suggestions for thresholds, providing research on what types of thresholds to choose, etc.</li>
                               <li>Alternately, provide information about how other parties can submit their statements on thresholds.</li>
                           </ol>
                       </li>
                       <li>Confirm that set thresholds align with any relevant international agreements. Once confirmed, affirm this in the safety framework.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 2.2 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 2.2 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 2.2 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 2.2 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Section 3 (no subsections) -->
        <div class="section-wrapper" data-section="3">
            <h2>Commitment 3 (Mitigations)</h2>
            <div class="info-box">
                <div class="info-box-header">
                    Frontier AI Safety Commitments, Commitment 3
                </div>
                <div class="info-box-content">
                    <p>III. Articulate how risk mitigations will be identified and implemented to keep risks within defined thresholds, including safety and security-related risk mitigations such as modifying system behaviours and implementing robust security controls for unreleased model weights.</p>
                </div>
            </div>
            <div class="content-and-aux-wrapper">
                <div class="content-block">
                    <ol>
                       <li>For security-related mitigations, discuss with your security team how they intend to prevent unauthorized access to model weights and other IP, and how they identified that strategy.
                           <ol>
                               <li>Document the strategy by which security mitigations were identified (not necessarily specific parts of your organization's security).</li>
                           </ol>
                       </li>
                       <li>For safety-related mitigations (typically called "deployment mitigations"), discuss with your engineering team how they intend to ensure that the model's behavior does not cause harm along the previously identified risk vectors, and how they chose that set of mitigations.
                           <ol>
                               <li>Document the strategy by which safety mitigations were identified.</li>
                               <li>Suggested questions to consider providing detail on:
                                   <ol>
                                       <li><em>What information sources are used to build the set of possible mitigations?</em></li>
                                       <li><em>How is the decision made to use or reject a given mitigation?</em></li>
                                       <li><em>Why</em> are these mitigations expected to reduce the risk from undesirable behaviors from the model?</li>
                                   </ol>
                               </li>
                           </ol>
                       </li>
                       <li>For both security and safety mitigations, describe how they will be implemented.
                           <ol>
                               <li>Who will implement them?</li>
                               <li>How will they be tested to ensure that the mitigation is actually having an effect?</li>
                           </ol>
                       </li>
                       <li>Describe how it will be verified that mitigations are in fact keeping risk below the desired thresholds.
                           <ol>
                               <li>This could be via re-running the assessments defined above. Different thresholds may be needed pre-mitigation vs. post-mitigation.</li>
                           </ol>
                       </li>
                    </ol>
                    <p>Ideally, a final product would be able to map, for every risk vector identified, which mitigations are deployed in order to keep that risk vector in check.</p>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Section 3 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Section 3 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Section 3 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Section 3 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Section 4 -->
        <div class="section-wrapper" data-section="4">
            <h2>Commitment 4 (Mitigation Strategy)</h2>
            
            <div class="subsection-wrapper" data-subsection="4.1">
                <div class="content-block">
                    <h3>Step 1: Responding to Threshold Breach</h3>
                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 4
                        </div>
                        <div class="info-box-content">
                            <p>IV. Set out explicit processes they intend to follow if their model or system poses risks that meet or exceed the pre-defined thresholds...</p>
                        </div>
                    </div>

                    <ol>
                       <li>For each relevant threshold, describe corresponding processes that will be taken if an AI model exceeds that threshold at a given point. Processes should be appropriate to the risk type and stage of training or deployment when the risk is discovered. Some examples:
                           <ol>
                               <li>If a model exceeds a certain capability threshold on a risk assessment in training, activate new security measures.</li>
                               <li>If trained safeguards cannot defend against adversarial fine-tuning and sufficiently dangerous capabilities exist, pivot from open-weights to closed-weights deployment.</li>
                               <li>If mitigations fail to reduce a model's performance on a risk assessment to below given thresholds, reorient research teams into identifying or developing more effective mitigations.</li>
                               <li>If new elicitation techniques show that an already-deployed model has dangerous capabilities, shift existing customers temporarily to a safer model until mitigations can be applied.</li>
                           </ol>
                       </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 4.1 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 4.1 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 4.1 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 4.1 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="4.2">
                <div class="content-block">
                    <h3>Step 2: Limits for Deployment and Development</h3>
                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 4
                        </div>
                        <div class="info-box-content">
                            <p>IV. [...] This includes processes to further develop and deploy their systems and models only if they assess that residual risks would stay below the thresholds. In the extreme, organisations commit not to develop or deploy a model or system at all, if mitigations cannot be applied to keep risks below the thresholds.</p>
                        </div>
                    </div>
                    <ol>
                       <li>Intolerable-risk thresholds from Commitment 2 will likely include scenarios in which a model cannot be deployed. If necessary, describe additional thresholds where a model cannot be safely developed. For example:
                           <ol>
                               <li>The model has human-expert-level cyber offensive capabilities, and security levels are inadequate to prevent top hacking organizations from stealing the model weights.</li>
                               <li>The model demonstrates high levels of autonomy, including skills such as goal-directedness and self-replication, and tends to try to self-improve across a variety of tasks, but the model is not thoroughly sandboxed and air-gapped.</li>
                           </ol>
                       </li>
                       <li>Describe what processes will be set in place to permit halting model development or deployment. Some questions to consider:
                           <ol>
                               <li>What stakeholders need to be consulted before aborting a planned deployment? Who makes the final decision?</li>
                               <li>To prepare for potential de-deployments, can the organization ensure readiness to switch all customers to a known safer model?</li>
                               <li>Will pauses on development happen automatically if dangerous capability evaluations trigger, or will they have a human-in-the-loop?</li>
                               <li>Once such processes have been triggered, what evidence will be required to continue with development or deployment?</li>
                           </ol>
                       </li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 4.2 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 4.2 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 4.2 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 4.2 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Section 5 (no subsections) -->
        <div class="section-wrapper" data-section="5">
            <h2>Commitment 5 (Continual Improvement)</h2>

            <div class="info-box">
                <div class="info-box-header">
                    Frontier AI Safety Commitments, Commitment 5
                </div>
                <div class="info-box-content">
                    <p>V. Continually invest in advancing their ability to implement commitments i-iv, including risk assessment and identification, thresholds definition, and mitigation effectiveness. This should include processes to assess and monitor the adequacy of mitigations, and identify additional mitigations as needed to ensure risks remain below the pre-defined thresholds. They will contribute to and take into account emerging best practice, international standards, and science on AI risk identification, assessment, and mitigation.</p>
                </div>
            </div>

            <div class="content-and-aux-wrapper">

                <div class="content-block">
                    <ol>
                       <li>Describe how the safety framework will be updated. If possible, describe how each of the above sections will be updated, i.e.:
                           <ol>
                               <li>How will new risk categories be added to the framework?</li>
                               <li>How will new risk assessments be designed and used?</li>
                               <li>How will risk thresholds be updated over time?</li>
                               <li>How will internal procedures be changed as AI progress advances?</li>
                           </ol>
                       </li>
                       <li>Ensure that the process of updating mitigations is thoroughly described, as the commitments pay a particular focus to mitigations.
                           <ol>
                               <li>How and when are mitigations tested to ensure that they continue to be effective on more advanced models?</li>
                               <li>How are new mitigations identified and evaluated?
                                   <ol>
                                       <li>Both these sections will likely draw heavily on already-established processes described for Commitment 3.</li>
                                   </ol>
                               </li>
                           </ol>
                       </li>
                       <li>Affirm that updates will take into account the various sources listed in the commitments.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Section 5 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Section 5 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Section 5 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Section 5 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Section 6 (no subsections) -->
        <div class="section-wrapper" data-section="6">
            <h2>Commitment 6 (Governance)</h2>

            <div class="info-box">
                <div class="info-box-header">
                    Frontier AI Safety Commitments, Commitment 6
                </div>
                <div class="info-box-content">
                    <p>VI. Adhere to the commitments outlined in I-V, including by developing and continuously reviewing internal accountability and governance frameworks and assigning roles, responsibilities and sufficient resources to do so.</p>
                </div>
            </div>


            <div class="content-and-aux-wrapper">

                <div class="content-block">
                    <ol>
                       <li>Identify key actors in the organization that are required for the safety framework to be successfully implemented. This involves, at a minimum:
                           <ol>
                               <li>Actors involved in risk assessment.</li>
                               <li>Actors involved in setting risk thresholds.</li>
                               <li>Actors involved in identifying and implementing risk mitigations.</li>
                               <li>Actors involved in making decisions about whether models are safe to develop or deploy.</li>
                               <li>Actors involved in updating and maintaining the safety framework.</li>
                               <li>Actors that ensure that the organization is adhering to the safety framework.</li>
                           </ol>
                       </li>
                       <li>Describe each of these groups, and their responsibilities with regards to the commitments made within the safety framework.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Section 6 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Section 6 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Section 6 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Section 6 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Section 7 -->
        <div class="section-wrapper" data-section="7">
            <h2>Commitment 7 (Transparency)</h2>
            
            <div class="subsection-wrapper" data-subsection="7.1">
                <div class="content-block">
                    <h3>Step 1: Prepare Framework for Publication</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 7
                        </div>
                        <div class="info-box-content">
                            <p>VII. Provide public transparency on the implementation of the above (I-VI), except insofar as doing so would increase risk or divulge sensitive commercial information to a degree disproportionate to the societal benefit...</p>
                        </div>
                    </div>

                    <ol>
                       <li>Step through the framework section-by-section.</li>
                       <li>In each section, identify any text that would leak commercially-valuable information without substantial benefit.
                           <ol>
                               <li>For example: details on specific security assessments and mitigations used, which could enable adversaries to better circumvent security systems.</li>
                               <li>As a counter-example of information that is commercially-valuable, but still potentially worth sharing: Information on which safety-mitigations are used could potentially be intellectual property, but sharing it could still yield substantial social benefit, as it would enable outside researchers to study the methods more and identify flaws and room for improvement.</li>
                           </ol>
                       </li>
                       <li>Produce a version of the framework without the identified information.</li>
                       <li>Publish the sanitized framework.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 7.1 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 7.1 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 7.1 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 7.1 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="subsection-wrapper" data-subsection="7.2">
                <div class="content-block">
                    <h3>Step 2: Enable Trusted Access</h3>

                    <div class="info-box">
                        <div class="info-box-header">
                            Frontier AI Safety Commitments, Commitment 7
                        </div>
                        <div class="info-box-content">
                            <p>VII. [...] They should still share more detailed information which cannot be shared publicly with trusted actors, including their respective home governments or appointed body, as appropriate.</p>
                        </div>
                    </div>

                    <ol>
                       <li>Similarly to before, step through the framework and identify any text that cannot be shared even with highly trusted actors.
                           <ol>
                               <li>For example: specific training details that have no bearing on the safety properties of a model.</li>
                           </ol>
                       </li>
                       <li>Produce a version of the framework without the identified information.</li>
                       <li>Identify which actors are trusted to view the more complete safety framework.</li>
                       <li>Affirm that trusted actors can gain access to this new version of the safety framework.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Subsection 7.2 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Subsection 7.2 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Subsection 7.2 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Subsection 7.2 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Section 8 (no subsections) -->
        <div class="section-wrapper" data-section="8">
            <h2>Commitment 8 (Feedback Opportunities)</h2>
            <div class="info-box">
                <div class="info-box-header">
                    Frontier AI Safety Commitments, Commitment 8
                </div>
                <div class="info-box-content">
                    <p>VIII. Explain how, if at all, external actors, such as governments, civil society, academics, and the public are involved in the process of assessing the risks of their AI models and systems, the adequacy of their safety framework (as described under I-VI), and their adherence to that framework.</p>
                </div>
            </div>

            <div class="content-and-aux-wrapper">
                <div class="content-block">
                    <ol>
                       <li>Create a way for external actors to give feedback and describe it in the public-facing safety framework.</li>
                       <li>Assign personnel to read the feedback and send summaries of its key contents for relevant people in the organization.</li>
                    </ol>
                </div>
                <div class="auxiliary-box">
                    <div class="tabs">
                        <button class="tab active" data-tab="1" data-type="typeA">Type A</button>
                        <button class="tab" data-tab="2" data-type="typeB">Type B</button>
                        <button class="tab" data-tab="3" data-type="typeC">Type C</button>
                        <button class="tab" data-tab="4" data-type="typeD">Type D</button>
                    </div>
                    <div class="tab-contents-wrapper">
                        <div class="tab-content active" data-tab="1" data-type="typeA">
                            <h3>Section 8 - Type A</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="2" data-type="typeB">
                            <h3>Section 8 - Type B</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="3" data-type="typeC">
                            <h3>Section 8 - Type C</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                        <div class="tab-content" data-tab="4" data-type="typeD">
                            <h3>Section 8 - Type D</h3>
                            <p>$FILLER_TEXT</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>